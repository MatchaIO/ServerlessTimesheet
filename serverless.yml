# For full config options, check the docs:
#    docs.serverless.com

service: matcha-serverless-timesheets 

provider:
  name: aws
  runtime: nodejs4.3 #TODO ready for an upgrade? v6 is available
  stage: dev
  region: us-east-1
  profile: serverless-rhystest_dev  
# you can add statements to the Lambda function's IAM Role here
  iamRoleStatements:
    - Effect: "Allow"
      Action:
        - "dynamodb:GetItem"
        - "dynamodb:GetRecords"
        - "dynamodb:PutItem"
        - "dynamodb:BatchWriteItem"
        - "dynamodb:Query"
        - "dynamodb:Scan"
        - "dynamodb:GetShardIterator"
        - "dynamodb:DescribeStream"
        - "dynamodb:ListStreams"
      Resource:
        #NB: Fn::Sub: conflicts with the string template that serverless uses, so it usually just easier not to use it
        - Fn::Join: ["", ["arn:aws:dynamodb:", {"Ref": "AWS::Region"}, ":", {"Ref": "AWS::AccountId"}, ":table/Timesheets"]]        
    - Effect: "Allow"
      Action:
        - "firehose:PutRecord"
        - "firehose:PutRecordBatch"
      Resource: 
        - Fn::Join: ["", ["arn:aws:firehose:", {"Ref": "AWS::Region"}, ":", {"Ref": "AWS::AccountId"}, ":deliverystream/", {"Ref": "DataFirehoseStream"}]]
    - Effect: "Allow"
      Action:
        - "logs:CreateLogGroup"
        - "logs:CreateLogStream"
        - "logs:PutLogEvents"
      Resource: 
        - 'Fn::Join':
          - ':'
          -
            - 'arn:aws:logs'
            - Ref: 'AWS::Region'
            - Ref: 'AWS::AccountId'
            - 'log-group:/aws/lambda/*:*:*' #TODO - we can proably do better than this too (it should only be for this service)
    - Effect: "Allow"
      Action:
        - "xray:PutTraceSegments"
        - "xray:PutTelemetryRecords"
      Resource: "*" #TODO HACK - Dont use * for a resource - get the template for the resource arn
      
# you can add packaging information here
package:
  exclude:
    - ./**
  include:
    - timesheetService/*.js

# https://github.com/serverless/serverless/blob/master/docs/02-providers/aws/events/01-apigateway.md
functions:
  createTimesheet:
    handler: timesheetService/httpHandler.createTimesheet
    events:
      - http: 
          path: timesheet
          method: POST
  updateTimesheet:
    handler: timesheetService/httpHandler.updateTimesheet
    events:
      - http: 
          path: timesheet/{id}
          method: PUT
  submitTimesheet:
    handler: timesheetService/httpHandler.submitTimesheet
    events:
      - http: 
          path: timesheet/{id}/submit
          method: POST
  eventStoreToDataLake:
    handler: timesheetService/eventStoreToDataLakeHandler.handler
    events:
      - stream:
          type: dynamodb
          arn: 
            Fn::GetAtt: [TimesheetDb, StreamArn]
          batchSize: 50
          startingPosition: TRIM_HORIZON
          enabled: false

resources:
  Resources:
    TimesheetDb:
      Type: 'AWS::DynamoDB::Table'
      DeletionPolicy: Retain
      Properties:
        AttributeDefinitions:
          - AttributeName: id
            AttributeType: S
          - AttributeName: timestamp
            AttributeType: N
        KeySchema:
          - AttributeName: id
            KeyType: HASH
          - AttributeName: timestamp
            KeyType: RANGE
        ProvisionedThroughput:
          ReadCapacityUnits: 1
          WriteCapacityUnits: 1
        StreamSpecification:
          StreamViewType: 'NEW_IMAGE'
        TableName: 'Timesheets'
    S3EventsBucketName:
      Type: AWS::S3::Bucket
      Properties:
        BucketName: "timesheets-firehose-bucket"
        VersioningConfiguration:
          Status: Suspended
    DataFirehoseStream:
      Type: "AWS::KinesisFirehose::DeliveryStream"
      Properties:
        DeliveryStreamName: "Timesheets-firehose"
        S3DestinationConfiguration:
          BucketARN: 
            Fn::Join: ["", ["arn:aws:s3:::", {"Ref": "S3EventsBucketName"}]]
          BufferingHints:
            IntervalInSeconds: 60
            SizeInMBs: 2
          CompressionFormat: UNCOMPRESSED #GZIP
          Prefix: "" # I dont think we should default a prefix, unless we decide to share buckets
          RoleARN:
            Fn::GetAtt: [ DataFirehoseToS3Role, Arn ]
          #EncryptionConfiguration:  EncryptionConfiguration - we can do this when we can manage keys
    DataFirehoseToS3Role:
      Type: "AWS::IAM::Role"
      Properties:
        RoleName: "Timesheets-firehose-to-s3-role"
        AssumeRolePolicyDocument:
          Version: "2012-10-17"
          Statement:
            -
              Effect: "Allow"
              Principal:
                Service:
                  - "firehose.amazonaws.com"
              Action: "sts:AssumeRole"
              Condition:
                StringEquals:
                  "sts:ExternalId":
                    Ref: "AWS::AccountId"
        Policies:
          -
            PolicyName: "Timesheets-firehose-to-s3-policy"
            PolicyDocument:
              Version: "2012-10-17"
              Statement:
                -
                  Effect: "Allow"
                  Action:
                    - "s3:PutObject"
                    - "s3:AbortMultipartUpload"
                    - "s3:GetBucketLocation"
                    - "s3:GetObject"
                    - "s3:ListBucket"
                    - "s3:ListBucketMultipartUploads"
                  Resource:
                    - Fn::Join: ["", ["arn:aws:s3:::", {"Ref": "S3EventsBucketName"}]]
                    - Fn::Join: ["", ["arn:aws:s3:::", {"Ref": "S3EventsBucketName"}, "/*"]]
                -
                  Effect: "Allow"
                  Action:
                    - "logs:CreateLogGroup"
                    - "logs:CreateLogStream"
                    - "logs:PutLogEvents"
                  Resource: "*"     
  Outputs:  
    TimesheetDbStreamArn:
      Description: Table Stream arn of the newly create TimesheetDb DynamoDB table
      Value:  
        Fn::GetAtt: [TimesheetDb, StreamArn]
      Export:
        Name: "timesheets-dynamodb-stream-arn"
    S3EventsBucketArn:
      Description: The data lake s3 bucket ARN that will contain all raw events
      Value:  
        Fn::Join: ["", ["arn:aws:s3:::", {"Ref": "S3EventsBucketName"}]]
      Export:
        Name: "timesheets-event-archive-s3-bucket-arn"