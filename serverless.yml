# Welcome to Serverless!
#
# This file is the main config file for your service.
# It's very minimal at this point and uses default values.
# You can always add more config options for more control.
# We've included some commented out config examples here.
# Just uncomment any of them to get that config option.
#
# For full config options, check the docs:
#    docs.serverless.com
#
# Happy Coding!

service: matcha-serverless-timesheets 

provider:
  name: aws
  runtime: nodejs4.3
  stage: dev
  region: ap-southeast-2
  profile: serverless-rhystest_dev
# you can add statements to the Lambda function's IAM Role here
  iamRoleStatements:
    - Effect: "Allow"
      Action:
        - "dynamodb:GetItem"
        - "dynamodb:GetRecords"
        - "dynamodb:PutItem"
        - "dynamodb:BatchWriteItem"
        - "dynamodb:Query"
        - "dynamodb:Scan"
        - "dynamodb:GetShardIterator"
        - "dynamodb:DescribeStream"
        - "dynamodb:ListStreams"
      Resource:
        - Fn::Sub: "arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/Timesheets"
    - Effect: "Allow"
      Action: 
        - "sts:AssumeRole"
      Principal:
        Service:
          - "lambda.amazonaws.com"
    - Effect: "Allow"
      Action:
        - "firehose:PutRecord"
        - "firehose:PutRecordBatch"
      Resource: 
        - Fn::Sub: "arn:aws:firehose:${AWS::Region}:${AWS::AccountId}:deliverystream/${DataFirehoseStream}"
    - Effect: "Allow"
      Action:
        - "logs:CreateLogGroup"
        - "logs:CreateLogStream"
        - "logs:PutLogEvents"
      Resource: "*"
    - Effect: "Allow"
      Action:
        - "xray:PutTraceSegments"
        - "xray:PutTelemetryRecords"
      Resource: "*"
      
# you can add packaging information here
package:
  exclude:
    - ./**
  include:
    - timesheetService/*.js

# https://github.com/serverless/serverless/blob/master/docs/02-providers/aws/events/01-apigateway.md
functions:
  createTimesheet:
    handler: timesheetService/httpHandler.createTimesheet
    events:
      - http: 
          path: timesheet
          method: POST
  updateTimesheet:
    handler: timesheetService/httpHandler.updateTimesheet
    events:
      - http: 
          path: timesheet/{id}
          method: PUT
  submitTimesheet:
    handler: timesheetService/httpHandler.submitTimesheet
    events:
      - http: 
          path: timesheet/{id}/submit
          method: POST
  eventStoreToDataLake:
    handler: timesheetService/eventStoreToDataLakeHandler.handler
    events:
      - stream:
          arn: Fn::GetAtt: [TimesheetDb, StreamArn]
          batchSize: 50
          startingPosition: TRIM_HORIZON
          enabled: false

resources:
  Resources:
    TimesheetDb:
      Type: 'AWS::DynamoDB::Table'
      DeletionPolicy: Retain
      Properties:
        AttributeDefinitions:
          - AttributeName: id
            AttributeType: S
          - AttributeName: timestamp
            AttributeType: N
        KeySchema:
          - AttributeName: id
            KeyType: HASH
          - AttributeName: timestamp
            KeyType: RANGE
        ProvisionedThroughput:
          ReadCapacityUnits: 1
          WriteCapacityUnits: 1
        StreamSpecification:
          StreamViewType: 'NEW_IMAGE'
        TableName: 'Timesheets'
    S3EventsBucketName:
      Type: AWS::S3::Bucket
      Properties:
        BucketName: "Timesheets-firehose-bucket"
        VersioningConfiguration:
          Status: Suspended
    DataFirehoseStream:
      Type: "AWS::KinesisFirehose::DeliveryStream"
      Properties:
        DeliveryStreamName: !Sub "Timesheets-firehose"
        S3DestinationConfiguration:
          BucketARN: 
            Fn::Sub: "arn:aws:s3:::${S3EventsBucketName}"
          BufferingHints:
            IntervalInSeconds: 60
            SizeInMBs: 2
          CompressionFormat: UNCOMPRESSED #GZIP
          Prefix: "" # I dont think we should default a prefix, unless we decide to share buckets
          RoleARN:
            Fn::GetAtt: [ DataFirehoseToS3Role, Arn ]
          #EncryptionConfiguration:  EncryptionConfiguration - we can do this when we can manage keys
    DataFirehoseToS3Role:
      Type: "AWS::IAM::Role"
      Properties:
        RoleName: "Timesheets-firehose-to-s3-role"
        AssumeRolePolicyDocument:
          Version: "2012-10-17"
          Statement:
            -
              Effect: "Allow"
              Principal:
                Service:
                  - "firehose.amazonaws.com"
              Action: "sts:AssumeRole"
              Condition:
                StringEquals:
                  "sts:ExternalId":
                    Ref: "AWS::AccountId"
        Policies:
          -
            PolicyName: "Timesheets-firehose-to-s3-policy"
            PolicyDocument:
              Version: "2012-10-17"
              Statement:
                -
                  Effect: "Allow"
                  Action:
                    - "s3:PutObject"
                    - "s3:AbortMultipartUpload"
                    - "s3:GetBucketLocation"
                    - "s3:GetObject"
                    - "s3:ListBucket"
                    - "s3:ListBucketMultipartUploads"
                  Resource:
                    - !Sub "arn:aws:s3:::${S3EventsBucketName}"
                    - !Sub "arn:aws:s3:::${S3EventsBucketName}/*"
                -
                  Effect: "Allow"
                  Action:
                    - "logs:CreateLogGroup"
                    - "logs:CreateLogStream"
                    - "logs:PutLogEvents"
                  Resource: "*"     
  Outputs:  
    TimesheetDbStreamArn:
      Description: Table Stream arn of the newly create TimesheetDb DynamoDB table
      Value:  
        Fn::GetAtt: [TimesheetDb, StreamArn]
      Export:
        Name: "timesheets-dynamodb-stream-arn"